{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below, you need to load the magellan-1.0.5-s_2.11.jar file as a local file\n",
    "using \"jars\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>1</td><td>None</td><td>spark</td><td>idle</td><td></td><td></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8d9f9692071454c84e7deff363aeb91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'jars': ['/home/lorenzo/EdgeCloudApproximate/libraries/magellan-1.0.5-s_2.11.jar'], 'conf': {'spark.jars.packages': 'org.apache.spark:spark-sql-kafka-0-10_2.11:2.2.0,com.esri.geometry:esri-geometry-api:1.2.1,commons-io:commons-io:2.6', 'spark.jars.excludes': 'org.scala-lang:scala-reflect,org.apache.spark:spark-tags_2.11', 'spark.dynamicAllocation.enabled': False}, 'kind': 'spark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>1</td><td>None</td><td>spark</td><td>idle</td><td></td><td></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%configure -f\n",
    "{\n",
    "    \"jars\":[\"/home/lorenzo/EdgeCloudApproximate/libraries/magellan-1.0.5-s_2.11.jar\"],\n",
    "    \"conf\": {\n",
    "        \"spark.jars.packages\": \"org.apache.spark:spark-sql-kafka-0-10_2.11:2.2.0,com.esri.geometry:esri-geometry-api:1.2.1,commons-io:commons-io:2.6\",\n",
    "        \"spark.jars.excludes\": \"org.scala-lang:scala-reflect,org.apache.spark:spark-tags_2.11\",\n",
    "        \n",
    "        \"spark.dynamicAllocation.enabled\": false\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "305f6216b2ac417687b7a965936754b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "/**\n",
    " * @Description georeferenced data online stratified sampling\n",
    " * @author Isam Al Jawarneh\n",
    " * @date 20/Jan/2023\n",
    " */"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d21eb75df254e2ea221d532b5c7e711",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "// sc.getConf.getAll.foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df8c9291212346bdbcde55576826c719",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "// sc.getConf.get(\"spark.dynamicAllocation.enabled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "948e31904d7a40be92082503c0704df2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import util.control.Breaks._\n",
      "import org.apache.spark.sql.streaming.StreamingQueryListener\n",
      "import org.apache.spark.sql.functions.col\n",
      "import org.apache.spark.sql.types._\n",
      "import org.apache.spark.rdd.RDD\n",
      "import org.apache.spark.SparkContext\n",
      "import org.apache.spark.SparkConf\n",
      "import org.apache.spark.sql.SparkSession\n",
      "import org.apache.spark.sql.types._\n",
      "import org.apache.spark.sql.SQLImplicits\n",
      "import org.apache.spark.sql.functions.from_json\n",
      "import org.apache.spark.sql.functions._\n",
      "import org.apache.spark.sql.DataFrame\n",
      "import org.apache.spark.sql.Dataset\n",
      "import org.apache.spark.sql.ForeachWriter\n",
      "import magellan._\n",
      "import magellan.index.ZOrderCurve\n",
      "import magellan.{Point, Polygon}\n",
      "import org.apache.spark.sql.magellan.dsl.expressions._\n",
      "import org.apache.spark.sql.Row\n",
      "import org.apache.spark.sql.types._\n",
      "import org.apache.spark.sql.SparkSession\n",
      "import org.apache.spark.sql.streaming.OutputMode\n",
      "import org.apache.spark.sql.types.{DoubleType, StringType, StructField, StructType}\n",
      "import org.apache.spark.sql.streaming._\n",
      "import org.apache.spark.sql.streaming.Trigger\n",
      "import org.apache.spark.sql.execution.streaming.MemoryStream\n",
      "import org.apache.spark.sql.functions.{collect_list, collect_set}\n",
      "import org.apache.spark.sql.SQLContext\n",
      "import org.apache.log4j.{Level, Logger}\n",
      "import scala.collection.mutable\n",
      "import scala.concurrent.duration.Duration\n",
      "import java.io.{BufferedWriter, FileWriter}\n",
      "import org.apache.commons.io.FileUtils\n",
      "import java.io.File\n",
      "import scala.collection.mutable.ListBuffer\n",
      "import java.time.Instant\n",
      "import org.apache.spark.sql.DataFrame\n",
      "import org.apache.kafka.clients.consumer.ConsumerRecord\n",
      "import org.apache.kafka.common.serialization.StringDeserializer\n"
     ]
    }
   ],
   "source": [
    "import util.control.Breaks._\n",
    "import org.apache.spark.sql.streaming.StreamingQueryListener\n",
    "import org.apache.spark.sql.functions.col\n",
    "import org.apache.spark.sql.types._\n",
    "import org.apache.spark.rdd.RDD\n",
    "import org.apache.spark.SparkContext\n",
    "import org.apache.spark.SparkConf\n",
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.sql.types._\n",
    "import org.apache.spark.sql.SQLImplicits\n",
    "import org.apache.spark.sql.functions.from_json\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.DataFrame\n",
    "import org.apache.spark.sql.Dataset\n",
    "import org.apache.spark.sql.ForeachWriter\n",
    "import magellan._\n",
    "import magellan.index.ZOrderCurve\n",
    "import magellan.{Point, Polygon}\n",
    "\n",
    "import org.apache.spark.sql.magellan.dsl.expressions._\n",
    "import org.apache.spark.sql.Row\n",
    "import org.apache.spark.sql.types._\n",
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.sql.streaming.OutputMode\n",
    "import org.apache.spark.sql.types.{\n",
    "  DoubleType,\n",
    "  StringType,\n",
    "  StructField,\n",
    "  StructType\n",
    "}\n",
    "import org.apache.spark.sql.streaming._\n",
    "import org.apache.spark.sql.streaming.Trigger\n",
    "import org.apache.spark.sql.execution.streaming.MemoryStream\n",
    "import org.apache.spark.sql.functions.{collect_list, collect_set}\n",
    "import org.apache.spark.sql.SQLContext\n",
    "import org.apache.log4j.{Level, Logger}\n",
    "import scala.collection.mutable\n",
    "import scala.concurrent.duration.Duration\n",
    "import java.io.{BufferedWriter, FileWriter}\n",
    "import org.apache.commons.io.FileUtils\n",
    "import java.io.File\n",
    "import scala.collection.mutable.ListBuffer\n",
    "import java.time.Instant\n",
    "// import org.apache.spark.util.CollectionAccumulator\n",
    "import org.apache.spark.sql.DataFrame\n",
    "import org.apache.kafka.clients.consumer.ConsumerRecord\n",
    "import org.apache.kafka.common.serialization.StringDeserializer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the following link to create topic in Kafka if the topic is not there already!\n",
    "```\n",
    "%%bash\n",
    "#create topic 'spatial' with 4 partitions\n",
    "export KafkaZookeepers=\"localhost:2181\"\n",
    "\n",
    "/home/kafka/kafka/bin/kafka-topics.sh --create --replication-factor 1 --partitions 4 --topic spatial1 --zookeeper $KafkaZookeepers\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "List the available Kafka topics\n",
    "```\n",
    "%%bash\n",
    "#list topics to check\n",
    "export KafkaZookeepers=\"localhost:2181\"\n",
    "/home/kafka/kafka/bin/kafka-topics.sh --list --zookeeper $KafkaZookeepers\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41f7936febe44aff87aaf1bc5252baf4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stream: org.apache.spark.sql.DataFrame = [key: binary, value: binary ... 5 more fields]\n"
     ]
    }
   ],
   "source": [
    "// Create Kafka stream \n",
    "val stream = spark.readStream.format(\"kafka\").option(\"kafka.bootstrap.servers\", \"localhost:9092\").option(\"startingOffsets\", \"earliest\").option(\"subscribe\", \"spatial1\").load() // .option(\"maxOffsetsPerTrigger\",2).option(\"startingOffsets\", \"earliest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da16820ff2c444b9a67da1c42c3babbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "schemaShenzhen: org.apache.spark.sql.types.StructType = StructType(StructField(id,StringType,false), StructField(lat,DoubleType,false), StructField(lon,DoubleType,false), StructField(time,StringType,false), StructField(speed,DoubleType,false))\n"
     ]
    }
   ],
   "source": [
    "val schemaShenzhen = StructType(Array(\n",
    "    StructField(\"id\", StringType, false),\n",
    "    StructField(\"lat\", DoubleType, false),\n",
    "    StructField(\"lon\", DoubleType, false),\n",
    "    StructField(\"time\", StringType, false),\n",
    "    StructField(\"speed\", DoubleType, false))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0f877f10c1b4583b8501202e1235b25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sampling_fraction: Double = 0.5\n",
      "precision: Int = 30\n",
      "error_bound: Double = 0.1\n"
     ]
    }
   ],
   "source": [
    "// parameters\n",
    "val sampling_fraction = 0.5\n",
    "val precision = 30\n",
    "val error_bound = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5642285673714925abe4c8e74d1586ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "geohashUDF: org.apache.spark.sql.expressions.UserDefinedFunction = UserDefinedFunction(<function1>,ArrayType(StringType,true),Some(List(ArrayType(org.apache.spark.sql.types.ZOrderCurveUDT@6f003e,true))))\n"
     ]
    }
   ],
   "source": [
    "// The following user defined function is required to convert ZOrderCurve to geohash (base32)\n",
    "val geohashUDF = udf{(curve: Seq[ZOrderCurve]) => curve.map(_.toBase32())}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Ramp-up: before running the following cell, generate some data from the CSV file then ingest by Kafka topic created previously"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that `precision` in the following code is a configurable parameter and it is the geohash precision.\n",
    "> Precision 30 means 6 characters , precision 25 means 5 characters (multiples of 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa7fbf0525134f4793f853e1d26652b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformationStream1: org.apache.spark.sql.DataFrame = [id: string, lat: double ... 3 more fields]\n",
      "ridesGeohashed: org.apache.spark.sql.DataFrame = [point: point, geohashArray: array<string> ... 1 more field]\n",
      "warning: there was one deprecation warning; re-run with -deprecation for details\n",
      "dataStream1: org.apache.spark.sql.DataFrame = [point: point, geohashArray: array<string> ... 2 more fields]\n",
      "transformationStream: org.apache.spark.sql.DataFrame = [point: point, speed: double ... 1 more field]\n"
     ]
    }
   ],
   "source": [
    "val transformationStream1 = stream.selectExpr(\"CAST(value AS STRING)\").as[(String)].select(from_json($\"value\", schemaShenzhen).as(\"data\")).select(\"data.*\")\n",
    "\n",
    "val ridesGeohashed = transformationStream1.withColumn(\"point\", point($\"lat\",$\"lon\")).withColumn(\"index\", $\"point\" index  precision).withColumn(\"geohashArray\", geohashUDF($\"index.curve\")).select( $\"point\",$\"geohashArray\",$\"speed\")\n",
    "val dataStream1 = ridesGeohashed.explode(\"geohashArray\", \"geohash\") { a: mutable.WrappedArray[String] => a }\n",
    "val transformationStream = dataStream1.select(\"point\", \"speed\", \"geohash\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8756194a297427cbbb457a379213add",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getSamplingRate1: (map: Map[String,Double], defaultValue: Double)org.apache.spark.sql.expressions.UserDefinedFunction\n",
      "spatialSampleBy: (neigh_geohashed_df: org.apache.spark.sql.DataFrame, points_geohashed_df: org.apache.spark.sql.DataFrame, samplingRatio: Double)org.apache.spark.sql.DataFrame\n"
     ]
    }
   ],
   "source": [
    "/* Random sampling from each stratum individually,\n",
    " * map consists of strata and percentage for each stratum */\n",
    "def getSamplingRate1(map: Map[String, Double], defaultValue: Double) = udf{\n",
    "  (geohash: String, rnd: Double) =>\n",
    "      rnd < map.getOrElse(geohash.asInstanceOf[String], 0.0)\n",
    "}\n",
    "\n",
    "// spatial stratified sampling\n",
    "def spatialSampleBy(neigh_geohashed_df:DataFrame, points_geohashed_df:DataFrame, samplingRatio: Double): DataFrame = {\n",
    "    val geoSeq: Seq[String] = neigh_geohashed_df.select(\"geohash\").distinct.rdd.map(r => r(0).asInstanceOf[String]).collect()\n",
    "    val map = Map(geoSeq map { a => a -> samplingRatio }: _*)\n",
    "\n",
    "    val tossAcoin = rand(7L)\n",
    "    val getSamplingRate = udf { (geohash: Any, rnd: Double) =>\n",
    "      rnd < map.getOrElse(geohash.asInstanceOf[String], 0.0)\n",
    "    }\n",
    "    val samplepointDF =  points_geohashed_df.filter(getSamplingRate1(map, 0.0)($\"geohash\", tossAcoin))\n",
    "    return samplepointDF\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7747d01b33ab43b3b83a0b2275c649f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "warning: there was one deprecation warning; re-run with -deprecation for details\n",
      "geohashedNeighborhoods: (geohashPrecision: Int, filePath: String)org.apache.spark.sql.DataFrame\n"
     ]
    }
   ],
   "source": [
    "def geohashedNeighborhoods(geohashPrecision: Int, filePath: String): DataFrame = {\n",
    "\n",
    "    import spark.implicits._\n",
    "    /* preparing the neighborhoods table (static table) ... getting geohashes covering for every neighborhood and \n",
    "    exploding it, so that each neighborhood has many geohashes */\n",
    "\n",
    "    // this will be executed only one time - batch mode \n",
    "    val rawNeighborhoods = spark.sqlContext.read.format(\"magellan\").option(\"type\", \"geojson\").load(filePath).select($\"polygon\", $\"metadata\"(\"NAME\").as(\"neighborhood\")).cache()\n",
    "\n",
    "    val neighborhoods = rawNeighborhoods.withColumn(\"index\", $\"polygon\" index geohashPrecision).select($\"polygon\", $\"index\", \n",
    "        $\"neighborhood\").cache()\n",
    "    print(neighborhoods.count())\n",
    "\n",
    "    val zorderIndexedNeighborhoods = neighborhoods.withColumn(\"index\", explode($\"index\")).select(\"polygon\", \"index.curve\", \"index.relation\",\"neighborhood\")\n",
    "    val geohashedNeighborhoods= neighborhoods.withColumn(\"geohashArray\", geohashUDF($\"index.curve\"))\n",
    "\n",
    "    val explodedgeohashedNeighborhoods = geohashedNeighborhoods.explode(\"geohashArray\", \"geohash\") { a: mutable.WrappedArray[String] => a }\n",
    "\n",
    "    //unit testing: explodedgeohashedNeighborhoods.show(10)\n",
    "\n",
    "    explodedgeohashedNeighborhoods\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b96f83fcd984097b1de8cebfd8fb5a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "// read geoJSON file containing Neighborhoods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44094f3c70fa44d8aac5eb2ca6f527b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lorenzoRepo: String = /home/lorenzo/EdgeCloudApproximate\n",
      "12geohashedNeigboors: org.apache.spark.sql.DataFrame = [polygon: polygon, index: array<struct<curve:zordercurve,relation:string>> ... 3 more fields]\n"
     ]
    }
   ],
   "source": [
    "val lorenzoRepo = \"/home/lorenzo/EdgeCloudApproximate\"\n",
    "val geohashedNeigboors = geohashedNeighborhoods(precision, \"/home/lorenzo/EdgeCloudApproximate/data/china/neighborhood\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1105d78a38b40628d99cf5dcc8e12ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samplepointDF_SSS: org.apache.spark.sql.DataFrame = [point: point, speed: double ... 1 more field]\n"
     ]
    }
   ],
   "source": [
    "/* careful here we perform online spatial sampling. this means sampling on-the-fly (OTF) \n",
    "as we are taking data directly from transformationStream, which by itself a transformation from a Kafka stream \n",
    "'stream'\n",
    "*/\n",
    "val samplepointDF_SSS = spatialSampleBy(geohashedNeigboors,transformationStream,sampling_fraction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26b9f2b28eba4f27bed5585f990a758f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samplingStatisticsDF: org.apache.spark.sql.DataFrame = [geohash: string, per_strat_mean: double]\n"
     ]
    }
   ],
   "source": [
    "val samplingStatisticsDF  = samplepointDF_SSS.groupBy($\"geohash\").agg(avg($\"speed\").as(\"per_strat_mean\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3eabc15dab0f4440a9bfdd62f3ca775d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "points_new: org.apache.spark.sql.streaming.StreamingQuery = org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@7557a48c\n"
     ]
    }
   ],
   "source": [
    "/*\n",
    "thereafter, we output data to a local in-memory sink\n",
    "to be able to perform queries locally over already-aggregated stream data.\n",
    "so, this way we are writing only sumamries in-memory, which is more effecient\n",
    "*/\n",
    "val points_new = samplingStatisticsDF.writeStream.queryName(\"queryTable\").format(\"memory\").outputMode(\"complete\").start()//outputMode(\"append\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa682d645cfc4923b15b0449d2537cd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "tail of empty list\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// check wether the stream is active\n",
    "points_new.isActive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c69db452f4114a3eaa6317266ba6dfb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "population: org.apache.spark.sql.DataFrame = [geohash: string, per_strat_mean: double]\n",
      "+-------+--------------+\n",
      "|geohash|per_strat_mean|\n",
      "+-------+--------------+\n",
      "| ws11p7|         8.625|\n",
      "+-------+--------------+\n",
      "only showing top 1 row\n",
      "\n",
      "tau_hat_str: Double = 8.625\n"
     ]
    }
   ],
   "source": [
    "val population = spark.sql(\"select * from queryTable\")\n",
    "population.createOrReplaceTempView(\"updates\")\n",
    "population.show(1) // Add an action to force Spark to evaluate the query\n",
    "val tau_hat_str = if (spark.sql(\"select count(*) from updates\").head().getLong(0) > 0) {\n",
    "  spark.sql(\"select per_strat_mean from updates\").head().getDouble(0)\n",
    "} else {\n",
    "  println(\"Query result is empty.\")\n",
    "  0.0 // or whatever default value you want to use\n",
    "}\n",
    "//val tau_hat_str = spark.sql(\"select per_strat_mean from updates\").head().getDouble(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a767238d05d645118f3e1c0e9c786792",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|geohash|    per_strat_mean|\n",
      "+-------+------------------+\n",
      "| ws11p7|             8.625|\n",
      "| ws10tn| 16.78723404255319|\n",
      "| ws1171|              10.8|\n",
      "| ws0cj3|              16.0|\n",
      "| ws11pk|12.238095238095237|\n",
      "| ws0crg|              23.8|\n",
      "| ws10un|40.666666666666664|\n",
      "| ws10u4| 18.77777777777778|\n",
      "| ws1078| 9.920388349514564|\n",
      "| ws10fz|              15.2|\n",
      "| ws10fh|11.785714285714286|\n",
      "| ws106e|18.685314685314687|\n",
      "| ws10s6|20.955555555555556|\n",
      "| ws1111| 30.38095238095238|\n",
      "| ws1098|10.560975609756097|\n",
      "| ws10mg|14.707317073170731|\n",
      "| ws0crz|              13.5|\n",
      "| ws10es|22.511111111111113|\n",
      "| ws0cnr|              21.0|\n",
      "| ws10mq| 9.645714285714286|\n",
      "| ws10ve|            12.625|\n",
      "| wecpbj|10.607142857142858|\n",
      "| ws109r|              12.0|\n",
      "| ws10kd| 20.17338003502627|\n",
      "| ws0brz| 37.86363636363637|\n",
      "| ws0cw6|30.666666666666668|\n",
      "| ws112y|              17.0|\n",
      "| ws0brk| 18.82191780821918|\n",
      "| ws0cjx|               5.0|\n",
      "| ws1159|              17.0|\n",
      "+-------+------------------+\n",
      "only showing top 30 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "population.show(30)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write Results to Csv File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ad3406bec5848699489c91c53259c86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import java.time.LocalDateTime\n",
      "import java.time.format.DateTimeFormatter\n",
      "outputDir: String = /home/lorenzo/results/\n",
      "filename: String = 2023-03-12_12-42__results.csv\n"
     ]
    }
   ],
   "source": [
    "import java.time.LocalDateTime\n",
    "import java.time.format.DateTimeFormatter\n",
    "\n",
    "// Define the output directory and filename\n",
    "val outputDir = \"/home/lorenzo/results/\"\n",
    "val filename = LocalDateTime.now.format(DateTimeFormatter.ofPattern(\"yyyy-MM-dd_HH-mm\")) + \"__results.csv\"\n",
    "\n",
    "// Write the DataFrame to a CSV file with the chosen filename\n",
    "population.write.format(\"csv\").option(\"header\", \"true\").mode(\"overwrite\").save(outputDir + filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80c32cbd7c5c413a8a9545850d489742",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "// points.stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad56ddb5df304b0e9b8304c10e39550d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "points_new.stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f4d3b1e84f64dc9b3fbf0a6fa94b61f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "// run the following cell if you only want to delete or mark for delection a Kafka topic"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to Delete Topic `spatial1` \n",
    "```\n",
    "%%bash\n",
    "/home/kafka/kafka/bin/kafka-topics.sh --zookeeper localhost:2181 --delete --topic spatial1\n",
    "#sudo systemctl restart kafka\n",
    "#sudo systemctl status kafka\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark",
   "language": "scala",
   "name": "sparkkernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".sc",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
