{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'jars': ['/home/lorenzo/EdgeCloudApproximate/libraries/magellan-1.0.5-s_2.11.jar'], 'conf': {'spark.jars.packages': 'org.apache.spark:spark-sql-kafka-0-10_2.11:2.2.0,com.esri.geometry:esri-geometry-api:1.2.1,commons-io:commons-io:2.6', 'spark.jars.excludes': 'org.scala-lang:scala-reflect,org.apache.spark:spark-tags_2.11', 'spark.dynamicAllocation.enabled': False}, 'kind': 'spark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "No active sessions."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%configure -f\n",
    "{\n",
    "    \"jars\":[\"/home/lorenzo/EdgeCloudApproximate/libraries/magellan-1.0.5-s_2.11.jar\"],\n",
    "    \"conf\": {\n",
    "        \"spark.jars.packages\": \"org.apache.spark:spark-sql-kafka-0-10_2.11:2.2.0,com.esri.geometry:esri-geometry-api:1.2.1,commons-io:commons-io:2.6\",\n",
    "        \"spark.jars.excludes\": \"org.scala-lang:scala-reflect,org.apache.spark:spark-tags_2.11\",\n",
    "        \n",
    "        \"spark.dynamicAllocation.enabled\": false\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>0</td><td>None</td><td>spark</td><td>idle</td><td></td><td></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "/* in the above cell, you need to load the magellan-1.0.5-s_2.11.jar file as a local file\n",
    "using \"jars\": */"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "/**\n",
    " * @Description: georeferenced data online stratified sampling\n",
    " * @author: Isam Al Jawarneh\n",
    " * @date: 20/Jan/2023\n",
    " */"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "// sc.getConf.getAll.foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "// sc.getConf.get(\"spark.dynamicAllocation.enabled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import util.control.Breaks._\n",
      "import org.apache.spark.sql.streaming.StreamingQueryListener\n",
      "import org.apache.spark.sql.functions.col\n",
      "import org.apache.spark.sql.types._\n",
      "import org.apache.spark.rdd.RDD\n",
      "import org.apache.spark.SparkContext\n",
      "import org.apache.spark.SparkConf\n",
      "import org.apache.spark.sql.SparkSession\n",
      "import org.apache.spark.sql.types._\n",
      "import org.apache.spark.sql.SQLImplicits\n",
      "import org.apache.spark.sql.functions.from_json\n",
      "import org.apache.spark.sql.functions._\n",
      "import org.apache.spark.sql.DataFrame\n",
      "import org.apache.spark.sql.Dataset\n",
      "import org.apache.spark.sql.ForeachWriter\n",
      "import magellan._\n",
      "import magellan.index.ZOrderCurve\n",
      "import magellan.{Point, Polygon}\n",
      "import org.apache.spark.sql.magellan.dsl.expressions._\n",
      "import org.apache.spark.sql.Row\n",
      "import org.apache.spark.sql.types._\n",
      "import org.apache.spark.sql.SparkSession\n",
      "import org.apache.spark.sql.streaming.OutputMode\n",
      "import org.apache.spark.sql.types.{DoubleType, StringType, StructField, StructType}\n",
      "import org.apache.spark.sql.streaming._\n",
      "import org.apache.spark.sql.streaming.Trigger\n",
      "import org.apache.spark.sql.execution.streaming.MemoryStream\n",
      "import org.apache.spark.sql.functions.{collect_list, collect_set}\n",
      "import org.apache.spark.sql.SQLContext\n",
      "import org.apache.log4j.{Level, Logger}\n",
      "import scala.collection.mutable\n",
      "import scala.concurrent.duration.Duration\n",
      "import java.io.{BufferedWriter, FileWriter}\n",
      "import org.apache.commons.io.FileUtils\n",
      "import java.io.File\n",
      "import scala.collection.mutable.ListBuffer\n",
      "import java.time.Instant\n",
      "import org.apache.spark.sql.DataFrame\n",
      "import org.apache.kafka.clients.consumer.ConsumerRecord\n",
      "import org.apache.kafka.common.serialization.StringDeserializer\n"
     ]
    }
   ],
   "source": [
    "import util.control.Breaks._\n",
    "import org.apache.spark.sql.streaming.StreamingQueryListener\n",
    "import org.apache.spark.sql.functions.col\n",
    "import org.apache.spark.sql.types._\n",
    "import org.apache.spark.rdd.RDD\n",
    "import org.apache.spark.SparkContext\n",
    "import org.apache.spark.SparkConf\n",
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.sql.types._\n",
    "import org.apache.spark.sql.SQLImplicits\n",
    "import org.apache.spark.sql.functions.from_json\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.DataFrame\n",
    "import org.apache.spark.sql.Dataset\n",
    "import org.apache.spark.sql.ForeachWriter\n",
    "import magellan._\n",
    "import magellan.index.ZOrderCurve\n",
    "import magellan.{Point, Polygon}\n",
    "\n",
    "import org.apache.spark.sql.magellan.dsl.expressions._\n",
    "import org.apache.spark.sql.Row\n",
    "import org.apache.spark.sql.types._\n",
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.sql.streaming.OutputMode\n",
    "import org.apache.spark.sql.types.{\n",
    "  DoubleType,\n",
    "  StringType,\n",
    "  StructField,\n",
    "  StructType\n",
    "}\n",
    "import org.apache.spark.sql.streaming._\n",
    "import org.apache.spark.sql.streaming.Trigger\n",
    "import org.apache.spark.sql.execution.streaming.MemoryStream\n",
    "import org.apache.spark.sql.functions.{collect_list, collect_set}\n",
    "import org.apache.spark.sql.SQLContext\n",
    "import org.apache.log4j.{Level, Logger}\n",
    "import scala.collection.mutable\n",
    "import scala.concurrent.duration.Duration\n",
    "import java.io.{BufferedWriter, FileWriter}\n",
    "import org.apache.commons.io.FileUtils\n",
    "import java.io.File\n",
    "import scala.collection.mutable.ListBuffer\n",
    "import java.time.Instant\n",
    "// import org.apache.spark.util.CollectionAccumulator\n",
    "import org.apache.spark.sql.DataFrame\n",
    "import org.apache.kafka.clients.consumer.ConsumerRecord\n",
    "import org.apache.kafka.common.serialization.StringDeserializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "// use the folloing link to create topic in Kafka if the topic is not there already!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%bash\n",
    "#create topic 'spatial' with 4 partitions\n",
    "export KafkaZookeepers=\"localhost:2181\"\n",
    "\n",
    "/home/kafka/kafka/bin/kafka-topics.sh --create --replication-factor 1 --partitions 4 --topic spatial1 --zookeeper $KafkaZookeepers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "// list the available Kafka topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "%%bash\n",
    "#list topics to check\n",
    "export KafkaZookeepers=\"localhost:2181\"\n",
    "/home/kafka/kafka/bin/kafka-topics.sh --list --zookeeper $KafkaZookeepers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// create Kafka stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stream: org.apache.spark.sql.DataFrame = [key: binary, value: binary ... 5 more fields]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "val stream = spark.readStream.format(\"kafka\").option(\"kafka.bootstrap.servers\", \"localhost:9092\").option(\"startingOffsets\", \"earliest\").option(\"subscribe\", \"spatial1\").load() // .option(\"maxOffsetsPerTrigger\",2).option(\"startingOffsets\", \"earliest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "schemaShenzhen: org.apache.spark.sql.types.StructType = StructType(StructField(id,StringType,false), StructField(lat,DoubleType,false), StructField(lon,DoubleType,false), StructField(time,StringType,false), StructField(speed,DoubleType,false))\n"
     ]
    }
   ],
   "source": [
    "val schemaShenzhen = StructType(Array(\n",
    "    StructField(\"id\", StringType, false),\n",
    "    StructField(\"lat\", DoubleType, false),\n",
    "    StructField(\"lon\", DoubleType, false),\n",
    "    StructField(\"time\", StringType, false),\n",
    "    StructField(\"speed\", DoubleType, false)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sampling_fraction: Double = 0.5\n",
      "precision: Int = 30\n",
      "error_bound: Double = 0.1\n"
     ]
    }
   ],
   "source": [
    "// parameters\n",
    "\n",
    "val sampling_fraction = 0.5\n",
    "val precision = 30\n",
    "val error_bound = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "// the following user defined function is required to convert ZOrderCurve to geohash base32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "geohashUDF: org.apache.spark.sql.expressions.UserDefinedFunction = UserDefinedFunction(<function1>,ArrayType(StringType,true),Some(List(ArrayType(org.apache.spark.sql.types.ZOrderCurveUDT@2a820cc3,true))))\n"
     ]
    }
   ],
   "source": [
    "val geohashUDF = udf{(curve: Seq[ZOrderCurve]) => curve.map(_.toBase32())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "// ramp - up: before running the following cell, generate some data from the CSV file then ingest by Kafka topic\n",
    "// created previously"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "// note that 'precision' in the following code is a configurable parameter and it is the geohash precision \n",
    "// precision 30 means 6 characters , precision 25 means 5 characters (multiples of 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformationStream1: org.apache.spark.sql.DataFrame = [id: string, lat: double ... 3 more fields]\n",
      "ridesGeohashed: org.apache.spark.sql.DataFrame = [point: point, geohashArray: array<string> ... 1 more field]\n",
      "warning: there was one deprecation warning; re-run with -deprecation for details\n",
      "dataStream1: org.apache.spark.sql.DataFrame = [point: point, geohashArray: array<string> ... 2 more fields]\n",
      "transformationStream: org.apache.spark.sql.DataFrame = [point: point, speed: double ... 1 more field]\n"
     ]
    }
   ],
   "source": [
    "val transformationStream1 = stream.selectExpr(\"CAST(value AS STRING)\").as[(String)].select(from_json($\"value\", schemaShenzhen).as(\"data\")).select(\"data.*\")\n",
    "\n",
    "val ridesGeohashed = transformationStream1.withColumn(\"point\", point($\"lat\",$\"lon\")).withColumn(\"index\", $\"point\" index  precision).withColumn(\"geohashArray\", geohashUDF($\"index.curve\")).select( $\"point\",$\"geohashArray\",$\"speed\")\n",
    "val dataStream1 = ridesGeohashed.explode(\"geohashArray\", \"geohash\") { a: mutable.WrappedArray[String] => a }\n",
    "val transformationStream = dataStream1.select(\"point\", \"speed\", \"geohash\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getSamplingRate1: (map: Map[String,Double], defaultValue: Double)org.apache.spark.sql.expressions.UserDefinedFunction\n",
      "spatialSampleBy: (neigh_geohashed_df: org.apache.spark.sql.DataFrame, points_geohashed_df: org.apache.spark.sql.DataFrame, samplingRatio: Double)org.apache.spark.sql.DataFrame\n"
     ]
    }
   ],
   "source": [
    "/* random sampling from each stratum individually,\n",
    "map consists of strata and percentage for each stratum\n",
    "*/\n",
    " def getSamplingRate1(map: Map[String, Double], defaultValue: Double) = udf{\n",
    "  (geohash: String, rnd: Double) =>\n",
    "      rnd < map.getOrElse(geohash.asInstanceOf[String], 0.0)\n",
    "}\n",
    "\n",
    "// spatial stratified sampling\n",
    "def spatialSampleBy(neigh_geohashed_df:DataFrame, points_geohashed_df:DataFrame, samplingRatio: Double): DataFrame = {\n",
    "    val geoSeq: Seq[String] = neigh_geohashed_df.select(\"geohash\").distinct.rdd.map(r => r(0).asInstanceOf[String]).collect()\n",
    "    val map = Map(geoSeq map { a => a -> samplingRatio }: _*)\n",
    "\n",
    "        val tossAcoin = rand(7L)\n",
    "    val getSamplingRate = udf { (geohash: Any, rnd: Double) =>\n",
    "      rnd < map.getOrElse(geohash.asInstanceOf[String], 0.0)\n",
    "    }\n",
    "val samplepointDF =  points_geohashed_df.filter(getSamplingRate1(map, 0.0)($\"geohash\", tossAcoin))\n",
    "    return samplepointDF}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "warning: there was one deprecation warning; re-run with -deprecation for details\n",
      "geohashedNeighborhoods: (geohashPrecision: Int, filePath: String)org.apache.spark.sql.DataFrame\n"
     ]
    }
   ],
   "source": [
    "def geohashedNeighborhoods(geohashPrecision: Int, filePath: String): DataFrame = \n",
    "\n",
    "{\n",
    "\n",
    "import spark.implicits._\n",
    "/* preparing the neighborhoods table (static table) .... getting geohashes covering for every neighborhood and \n",
    "exploding it, so that each neighborhood has many geohashes */\n",
    "\n",
    "// this will be executed only one time - batch mode \n",
    "val rawNeighborhoods = spark.sqlContext.read.format(\"magellan\").option(\"type\", \"geojson\").load(filePath).select($\"polygon\", $\"metadata\"(\"NAME\").as(\"neighborhood\")).cache()\n",
    "\n",
    "val neighborhoods = rawNeighborhoods.withColumn(\"index\", $\"polygon\" index geohashPrecision).select($\"polygon\", $\"index\", \n",
    "      $\"neighborhood\").cache()\n",
    "    print(neighborhoods.count())\n",
    "\n",
    "val zorderIndexedNeighborhoods = neighborhoods.withColumn(\"index\", explode($\"index\")).select(\"polygon\", \"index.curve\", \"index.relation\",\"neighborhood\")\n",
    "val geohashedNeighborhoods= neighborhoods.withColumn(\"geohashArray\", geohashUDF($\"index.curve\"))\n",
    "\n",
    "val explodedgeohashedNeighborhoods = geohashedNeighborhoods.explode(\"geohashArray\", \"geohash\") { a: mutable.WrappedArray[String] => a }\n",
    "\n",
    "//unit testing: explodedgeohashedNeighborhoods.show(10)\n",
    "\n",
    "\n",
    "explodedgeohashedNeighborhoods\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "// read geoJSON file containing Neighborhoods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lorenzoRepo: String = /home/lorenzo/EdgeCloudApproximate\n",
      "12geohashedNeigboors: org.apache.spark.sql.DataFrame = [polygon: polygon, index: array<struct<curve:zordercurve,relation:string>> ... 3 more fields]\n"
     ]
    }
   ],
   "source": [
    "val lorenzoRepo = \"/home/lorenzo/EdgeCloudApproximate\"\n",
    "val geohashedNeigboors = geohashedNeighborhoods(precision, \"/home/lorenzo/EdgeCloudApproximate/data/china/neighborhood\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samplepointDF_SSS: org.apache.spark.sql.DataFrame = [point: point, speed: double ... 1 more field]\n"
     ]
    }
   ],
   "source": [
    "/* careful here we perform online spatial sampling. this means sampling on-the-fly (OTF) \n",
    "as we are taking data directly from transformationStream, which by itself a transformation from a Kafka stream \n",
    "'stream'\n",
    "*/\n",
    "val samplepointDF_SSS = spatialSampleBy(geohashedNeigboors,transformationStream,sampling_fraction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samplingStatisticsDF: org.apache.spark.sql.DataFrame = [geohash: string, per_strat_mean: double]\n"
     ]
    }
   ],
   "source": [
    "val samplingStatisticsDF  = samplepointDF_SSS.groupBy($\"geohash\").agg(avg($\"speed\").as(\"per_strat_mean\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "points_new: org.apache.spark.sql.streaming.StreamingQuery = org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@3dc3d4c3\n"
     ]
    }
   ],
   "source": [
    "/*\n",
    "thereafter, we output data to a local in-memory sink\n",
    "to be able to perform queries locally over already-aggregated stream data.\n",
    "so, this way we are writing only sumamries in-memory, which is more effecient\n",
    "*/\n",
    "val points_new = samplingStatisticsDF.writeStream.queryName(\"queryTable\").format(\"memory\").outputMode(\"complete\").start()//outputMode(\"append\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res70: Boolean = true\n"
     ]
    }
   ],
   "source": [
    "// check wether the stream is active\n",
    "points_new.isActive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "population: org.apache.spark.sql.DataFrame = [geohash: string, per_strat_mean: double]\n",
      "tau_hat_str: Double = 11.4\n"
     ]
    }
   ],
   "source": [
    "val population = spark.sql(\"select * from queryTable\")\n",
    "          \n",
    "population.createOrReplaceTempView(\"updates\")\n",
    "val tau_hat_str = spark.sql(\"select per_strat_mean from updates\").head().getDouble(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|geohash|    per_strat_mean|\n",
      "+-------+------------------+\n",
      "| ws11p7|              11.4|\n",
      "| ws10tn| 22.61904761904762|\n",
      "| ws1171|11.583333333333334|\n",
      "| ws11pk| 6.416666666666667|\n",
      "| ws0crg|              27.0|\n",
      "| ws10un|              41.0|\n",
      "| ws10u4| 18.77777777777778|\n",
      "| ws1078| 9.497098646034816|\n",
      "| ws10fz|              10.0|\n",
      "| ws10fh|              13.2|\n",
      "| ws106e|16.044025157232703|\n",
      "| ws10s6|17.136363636363637|\n",
      "| ws1111| 22.90909090909091|\n",
      "| ws1098| 8.857142857142858|\n",
      "| ws10mg|14.394160583941606|\n",
      "| ws0crz|              13.5|\n",
      "| ws10es|15.321428571428571|\n",
      "| ws10mq| 9.264367816091953|\n",
      "| ws10ve|               9.0|\n",
      "| wecpbj| 9.212389380530974|\n",
      "| ws109r|              12.0|\n",
      "| ws10kd|20.890845070422536|\n",
      "| ws0brz|37.093023255813954|\n",
      "| ws0cw6|30.333333333333332|\n",
      "| ws10hz| 7.662904439428141|\n",
      "| ws112y|              17.0|\n",
      "| ws0brk|20.414285714285715|\n",
      "| ws0cq6|16.166666666666668|\n",
      "| ws0bzs|              50.2|\n",
      "| ws1142| 6.333333333333333|\n",
      "+-------+------------------+\n",
      "only showing top 30 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "population.show(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "// points.stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "// run the following cell if you only want to delete or mark for delection a Kafka topic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%bash\n",
    "/home/kafka/kafka/bin/kafka-topics.sh --zookeeper localhost:2181 --delete --topic spatial1\n",
    "#sudo systemctl restart kafka\n",
    "#sudo systemctl status kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "points_new.stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark",
   "language": "scala",
   "name": "sparkkernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".sc",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
