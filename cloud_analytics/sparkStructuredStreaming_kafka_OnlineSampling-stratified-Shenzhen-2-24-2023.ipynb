{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below, you need to load the magellan-1.0.5-s_2.11.jar file as a local file\n",
    "using \"jars\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'jars': ['/home/lorenzo/EdgeCloudApproximate/libraries/magellan-1.0.5-s_2.11.jar'], 'conf': {'spark.jars.packages': 'org.apache.spark:spark-sql-kafka-0-10_2.11:2.2.0,com.esri.geometry:esri-geometry-api:1.2.1,commons-io:commons-io:2.6', 'spark.jars.excludes': 'org.scala-lang:scala-reflect,org.apache.spark:spark-tags_2.11', 'spark.dynamicAllocation.enabled': False}, 'kind': 'spark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>0</td><td>None</td><td>spark</td><td>idle</td><td></td><td></td><td>None</td><td></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%configure -f\n",
    "{\n",
    "    \"jars\":[\"/home/lorenzo/EdgeCloudApproximate/libraries/magellan-1.0.5-s_2.11.jar\"],\n",
    "    \"conf\": {\n",
    "        \"spark.jars.packages\": \"org.apache.spark:spark-sql-kafka-0-10_2.11:2.2.0,com.esri.geometry:esri-geometry-api:1.2.1,commons-io:commons-io:2.6\",\n",
    "        \"spark.jars.excludes\": \"org.scala-lang:scala-reflect,org.apache.spark:spark-tags_2.11\",\n",
    "        \n",
    "        \"spark.dynamicAllocation.enabled\": false\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>4</td><td>None</td><td>spark</td><td>idle</td><td></td><td></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f49c34f4ade41a9905984632b647e9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b960d28822e04d0c97f60f7a273aca98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "/**\n",
    " * @Description georeferenced data online stratified sampling\n",
    " * @author Isam Al Jawarneh\n",
    " * @date 20/Jan/2023\n",
    " */"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59f603b697884337b940838844f880d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "// sc.getConf.getAll.foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b28193c33d534f35a05603a6466a2853",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "// sc.getConf.get(\"spark.dynamicAllocation.enabled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cba436aeb51142c1bfc6a2bacfba57e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import util.control.Breaks._\n",
      "import org.apache.spark.sql.streaming.StreamingQueryListener\n",
      "import org.apache.spark.sql.functions.col\n",
      "import org.apache.spark.sql.types._\n",
      "import org.apache.spark.rdd.RDD\n",
      "import org.apache.spark.SparkContext\n",
      "import org.apache.spark.SparkConf\n",
      "import org.apache.spark.sql.SparkSession\n",
      "import org.apache.spark.sql.types._\n",
      "import org.apache.spark.sql.SQLImplicits\n",
      "import org.apache.spark.sql.functions.from_json\n",
      "import org.apache.spark.sql.functions._\n",
      "import org.apache.spark.sql.DataFrame\n",
      "import org.apache.spark.sql.Dataset\n",
      "import org.apache.spark.sql.ForeachWriter\n",
      "import magellan._\n",
      "import magellan.index.ZOrderCurve\n",
      "import magellan.{Point, Polygon}\n",
      "import org.apache.spark.sql.magellan.dsl.expressions._\n",
      "import org.apache.spark.sql.Row\n",
      "import org.apache.spark.sql.types._\n",
      "import org.apache.spark.sql.SparkSession\n",
      "import org.apache.spark.sql.streaming.OutputMode\n",
      "import org.apache.spark.sql.types.{DoubleType, StringType, StructField, StructType}\n",
      "import org.apache.spark.sql.streaming._\n",
      "import org.apache.spark.sql.streaming.Trigger\n",
      "import org.apache.spark.sql.execution.streaming.MemoryStream\n",
      "import org.apache.spark.sql.functions.{collect_list, collect_set}\n",
      "import org.apache.spark.sql.SQLContext\n",
      "import org.apache.log4j.{Level, Logger}\n",
      "import scala.collection.mutable\n",
      "import scala.concurrent.duration.Duration\n",
      "import java.io.{BufferedWriter, FileWriter}\n",
      "import org.apache.commons.io.FileUtils\n",
      "import java.io.File\n",
      "import scala.collection.mutable.ListBuffer\n",
      "import java.time.Instant\n",
      "import org.apache.spark.sql.DataFrame\n",
      "import org.apache.kafka.clients.consumer.ConsumerRecord\n",
      "import org.apache.kafka.common.serialization.StringDeserializer\n"
     ]
    }
   ],
   "source": [
    "import util.control.Breaks._\n",
    "import org.apache.spark.sql.streaming.StreamingQueryListener\n",
    "import org.apache.spark.sql.functions.col\n",
    "import org.apache.spark.sql.types._\n",
    "import org.apache.spark.rdd.RDD\n",
    "import org.apache.spark.SparkContext\n",
    "import org.apache.spark.SparkConf\n",
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.sql.types._\n",
    "import org.apache.spark.sql.SQLImplicits\n",
    "import org.apache.spark.sql.functions.from_json\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.DataFrame\n",
    "import org.apache.spark.sql.Dataset\n",
    "import org.apache.spark.sql.ForeachWriter\n",
    "import magellan._\n",
    "import magellan.index.ZOrderCurve\n",
    "import magellan.{Point, Polygon}\n",
    "\n",
    "import org.apache.spark.sql.magellan.dsl.expressions._\n",
    "import org.apache.spark.sql.Row\n",
    "import org.apache.spark.sql.types._\n",
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.sql.streaming.OutputMode\n",
    "import org.apache.spark.sql.types.{\n",
    "  DoubleType,\n",
    "  StringType,\n",
    "  StructField,\n",
    "  StructType\n",
    "}\n",
    "import org.apache.spark.sql.streaming._\n",
    "import org.apache.spark.sql.streaming.Trigger\n",
    "import org.apache.spark.sql.execution.streaming.MemoryStream\n",
    "import org.apache.spark.sql.functions.{collect_list, collect_set}\n",
    "import org.apache.spark.sql.SQLContext\n",
    "import org.apache.log4j.{Level, Logger}\n",
    "import scala.collection.mutable\n",
    "import scala.concurrent.duration.Duration\n",
    "import java.io.{BufferedWriter, FileWriter}\n",
    "import org.apache.commons.io.FileUtils\n",
    "import java.io.File\n",
    "import scala.collection.mutable.ListBuffer\n",
    "import java.time.Instant\n",
    "// import org.apache.spark.util.CollectionAccumulator\n",
    "import org.apache.spark.sql.DataFrame\n",
    "import org.apache.kafka.clients.consumer.ConsumerRecord\n",
    "import org.apache.kafka.common.serialization.StringDeserializer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the following link to create topic in Kafka if the topic is not there already!\n",
    "```\n",
    "%%bash\n",
    "#create topic 'spatial' with 4 partitions\n",
    "export KafkaZookeepers=\"localhost:2181\"\n",
    "\n",
    "/home/kafka/kafka/bin/kafka-topics.sh --create --replication-factor 1 --partitions 4 --topic spatial1 --zookeeper $KafkaZookeepers\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "List the available Kafka topics\n",
    "```\n",
    "%%bash\n",
    "#list topics to check\n",
    "export KafkaZookeepers=\"localhost:2181\"\n",
    "/home/kafka/kafka/bin/kafka-topics.sh --list --zookeeper $KafkaZookeepers\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62e77ebbbd484ab096de3888d5857e35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stream: org.apache.spark.sql.DataFrame = [key: binary, value: binary ... 5 more fields]\n"
     ]
    }
   ],
   "source": [
    "// Create Kafka stream \n",
    "val stream = spark.readStream.format(\"kafka\").option(\"kafka.bootstrap.servers\", \"localhost:9092\").option(\"startingOffsets\", \"earliest\").option(\"subscribe\", \"spatial1\").load() // .option(\"maxOffsetsPerTrigger\",2).option(\"startingOffsets\", \"earliest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02b9a4e9e59e4e4f953e5ee93c832c66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "schemaShenzhen: org.apache.spark.sql.types.StructType = StructType(StructField(id,StringType,false), StructField(lat,DoubleType,false), StructField(lon,DoubleType,false), StructField(time,StringType,false), StructField(speed,DoubleType,false))\n"
     ]
    }
   ],
   "source": [
    "val schemaShenzhen = StructType(Array(\n",
    "    StructField(\"id\", StringType, false),\n",
    "    StructField(\"lat\", DoubleType, false),\n",
    "    StructField(\"lon\", DoubleType, false),\n",
    "    StructField(\"time\", StringType, false),\n",
    "    StructField(\"speed\", DoubleType, false))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0d146037feb4103a0af9dde176a65bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sampling_fraction: Double = 0.5\n",
      "precision: Int = 30\n",
      "error_bound: Double = 0.1\n"
     ]
    }
   ],
   "source": [
    "// parameters\n",
    "val sampling_fraction = 0.5\n",
    "val precision = 30\n",
    "val error_bound = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c066a628093f4235b07d7be938aa072c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "geohashUDF: org.apache.spark.sql.expressions.UserDefinedFunction = UserDefinedFunction(<function1>,ArrayType(StringType,true),Some(List(ArrayType(org.apache.spark.sql.types.ZOrderCurveUDT@4c312b1d,true))))\n"
     ]
    }
   ],
   "source": [
    "// The following user defined function is required to convert ZOrderCurve to geohash (base32)\n",
    "val geohashUDF = udf{(curve: Seq[ZOrderCurve]) => curve.map(_.toBase32())}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Ramp-up: before running the following cell, generate some data from the CSV file then ingest by Kafka topic created previously"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that `precision` in the following code is a configurable parameter and it is the geohash precision.\n",
    "> Precision 30 means 6 characters , precision 25 means 5 characters (multiples of 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16fceabd198d49c9ab8e970d8c004b40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformationStream1: org.apache.spark.sql.DataFrame = [id: string, lat: double ... 3 more fields]\n",
      "ridesGeohashed: org.apache.spark.sql.DataFrame = [point: point, geohashArray: array<string> ... 1 more field]\n",
      "warning: there was one deprecation warning; re-run with -deprecation for details\n",
      "dataStream1: org.apache.spark.sql.DataFrame = [point: point, geohashArray: array<string> ... 2 more fields]\n",
      "transformationStream: org.apache.spark.sql.DataFrame = [point: point, speed: double ... 1 more field]\n"
     ]
    }
   ],
   "source": [
    "val transformationStream1 = stream.selectExpr(\"CAST(value AS STRING)\").as[(String)].select(from_json($\"value\", schemaShenzhen).as(\"data\")).select(\"data.*\")\n",
    "\n",
    "val ridesGeohashed = transformationStream1.withColumn(\"point\", point($\"lat\",$\"lon\")).withColumn(\"index\", $\"point\" index  precision).withColumn(\"geohashArray\", geohashUDF($\"index.curve\")).select( $\"point\",$\"geohashArray\",$\"speed\")\n",
    "val dataStream1 = ridesGeohashed.explode(\"geohashArray\", \"geohash\") { a: mutable.WrappedArray[String] => a }\n",
    "val transformationStream = dataStream1.select(\"point\", \"speed\", \"geohash\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed72d92b9fb94c6c8eef1b4dcb7a4e6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getSamplingRate1: (map: Map[String,Double], defaultValue: Double)org.apache.spark.sql.expressions.UserDefinedFunction\n",
      "spatialSampleBy: (neigh_geohashed_df: org.apache.spark.sql.DataFrame, points_geohashed_df: org.apache.spark.sql.DataFrame, samplingRatio: Double)org.apache.spark.sql.DataFrame\n"
     ]
    }
   ],
   "source": [
    "/* Random sampling from each stratum individually,\n",
    " * map consists of strata and percentage for each stratum */\n",
    "def getSamplingRate1(map: Map[String, Double], defaultValue: Double) = udf{\n",
    "  (geohash: String, rnd: Double) =>\n",
    "      rnd < map.getOrElse(geohash.asInstanceOf[String], 0.0)\n",
    "}\n",
    "\n",
    "// spatial stratified sampling\n",
    "def spatialSampleBy(neigh_geohashed_df:DataFrame, points_geohashed_df:DataFrame, samplingRatio: Double): DataFrame = {\n",
    "    val geoSeq: Seq[String] = neigh_geohashed_df.select(\"geohash\").distinct.rdd.map(r => r(0).asInstanceOf[String]).collect()\n",
    "    val map = Map(geoSeq map { a => a -> samplingRatio }: _*)\n",
    "\n",
    "    val tossAcoin = rand(7L)\n",
    "    val getSamplingRate = udf { (geohash: Any, rnd: Double) =>\n",
    "      rnd < map.getOrElse(geohash.asInstanceOf[String], 0.0)\n",
    "    }\n",
    "    val samplepointDF =  points_geohashed_df.filter(getSamplingRate1(map, 0.0)($\"geohash\", tossAcoin))\n",
    "    return samplepointDF\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bc4b7533d6542f2b6d690425daa9cdb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "warning: there was one deprecation warning; re-run with -deprecation for details\n",
      "geohashedNeighborhoods: (geohashPrecision: Int, filePath: String)org.apache.spark.sql.DataFrame\n"
     ]
    }
   ],
   "source": [
    "def geohashedNeighborhoods(geohashPrecision: Int, filePath: String): DataFrame = {\n",
    "\n",
    "    import spark.implicits._\n",
    "    /* preparing the neighborhoods table (static table) ... getting geohashes covering for every neighborhood and \n",
    "    exploding it, so that each neighborhood has many geohashes */\n",
    "\n",
    "    // this will be executed only one time - batch mode \n",
    "    val rawNeighborhoods = spark.sqlContext.read.format(\"magellan\").option(\"type\", \"geojson\").load(filePath).select($\"polygon\", $\"metadata\"(\"NAME\").as(\"neighborhood\")).cache()\n",
    "\n",
    "    val neighborhoods = rawNeighborhoods.withColumn(\"index\", $\"polygon\" index geohashPrecision).select($\"polygon\", $\"index\", \n",
    "        $\"neighborhood\").cache()\n",
    "    print(neighborhoods.count())\n",
    "\n",
    "    val zorderIndexedNeighborhoods = neighborhoods.withColumn(\"index\", explode($\"index\")).select(\"polygon\", \"index.curve\", \"index.relation\",\"neighborhood\")\n",
    "    val geohashedNeighborhoods= neighborhoods.withColumn(\"geohashArray\", geohashUDF($\"index.curve\"))\n",
    "\n",
    "    val explodedgeohashedNeighborhoods = geohashedNeighborhoods.explode(\"geohashArray\", \"geohash\") { a: mutable.WrappedArray[String] => a }\n",
    "\n",
    "    //unit testing: explodedgeohashedNeighborhoods.show(10)\n",
    "\n",
    "    explodedgeohashedNeighborhoods\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f9346e4150444589c1f1ad186a6cb18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "// read geoJSON file containing Neighborhoods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25b7772e0d374e628c95df8d137e2dc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lorenzoRepo: String = /home/lorenzo/EdgeCloudApproximate\n",
      "12geohashedNeigboors: org.apache.spark.sql.DataFrame = [polygon: polygon, index: array<struct<curve:zordercurve,relation:string>> ... 3 more fields]\n"
     ]
    }
   ],
   "source": [
    "val lorenzoRepo = \"/home/lorenzo/EdgeCloudApproximate\"\n",
    "val geohashedNeigboors = geohashedNeighborhoods(precision, \"/home/lorenzo/EdgeCloudApproximate/data/china/neighborhood\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "390b0dfd2c7142c5b5d26eecace63ca8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samplepointDF_SSS: org.apache.spark.sql.DataFrame = [point: point, speed: double ... 1 more field]\n"
     ]
    }
   ],
   "source": [
    "/* careful here we perform online spatial sampling. this means sampling on-the-fly (OTF) \n",
    "as we are taking data directly from transformationStream, which by itself a transformation from a Kafka stream \n",
    "'stream'\n",
    "*/\n",
    "val samplepointDF_SSS = spatialSampleBy(geohashedNeigboors,transformationStream,sampling_fraction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e41ca6b8ccbf46c89968292fa66e8194",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samplingStatisticsDF: org.apache.spark.sql.DataFrame = [geohash: string, per_strat_mean: double]\n"
     ]
    }
   ],
   "source": [
    "val samplingStatisticsDF  = samplepointDF_SSS.groupBy($\"geohash\").agg(avg($\"speed\").as(\"per_strat_mean\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76d345b856d8423bb17ce65fed009323",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "points_new: org.apache.spark.sql.streaming.StreamingQuery = org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@1f5df850\n"
     ]
    }
   ],
   "source": [
    "/*\n",
    "thereafter, we output data to a local in-memory sink\n",
    "to be able to perform queries locally over already-aggregated stream data.\n",
    "so, this way we are writing only sumamries in-memory, which is more effecient\n",
    "*/\n",
    "val points_new = samplingStatisticsDF.writeStream.queryName(\"queryTable\").format(\"memory\").outputMode(\"complete\").start()//outputMode(\"append\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd211c4cf2dc45caa1c89d2a463dcf8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res33: Boolean = true\n"
     ]
    }
   ],
   "source": [
    "// check wether the stream is active\n",
    "points_new.isActive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "458739b277fa4a18a375df00d5a6348a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "population: org.apache.spark.sql.DataFrame = [geohash: string, per_strat_mean: double]\n",
      "tau_hat_str: Double = 17.0\n"
     ]
    }
   ],
   "source": [
    "val population = spark.sql(\"select * from queryTable\")\n",
    "          \n",
    "population.createOrReplaceTempView(\"updates\")\n",
    "val tau_hat_str = spark.sql(\"select per_strat_mean from updates\").head().getDouble(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b80c10f87627498ca7074bf774e723c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|geohash|    per_strat_mean|\n",
      "+-------+------------------+\n",
      "| ws1078|              17.0|\n",
      "| ws106e|              25.0|\n",
      "| ws10es|              42.0|\n",
      "| ws10kd|              10.5|\n",
      "| ws0brz|               0.0|\n",
      "| ws10hz| 7.285714285714286|\n",
      "| ws102f|               6.5|\n",
      "| ws102k|               8.0|\n",
      "| ws0bpz|              23.0|\n",
      "| ws10sv|              28.0|\n",
      "| ws102g|              44.5|\n",
      "| ws100p|              18.0|\n",
      "| ws1037|              42.0|\n",
      "| ws107h| 7.142857142857143|\n",
      "| ws10cv|              40.0|\n",
      "| ws101t|               8.0|\n",
      "| ws101p|               8.5|\n",
      "| ws100r|              11.0|\n",
      "| ws107t|              18.0|\n",
      "| ws107s|              15.0|\n",
      "| ws0brc|              30.0|\n",
      "| ws1074|11.555555555555555|\n",
      "| ws1113|              25.0|\n",
      "| ws100e|               0.0|\n",
      "| ws105r|              33.0|\n",
      "| ws1007|               4.4|\n",
      "| ws1056|19.333333333333332|\n",
      "| ws0brj|               4.0|\n",
      "| ws10d0|              31.0|\n",
      "| ws100u|              42.0|\n",
      "+-------+------------------+\n",
      "only showing top 30 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "population.show(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57c06be9d6234feca58ce1cd6912d3bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "// points.stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84dbe08ca2f84dec8237792776a725ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "points_new.stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d98e4a7d3704a6d927a58ac6fe2e062",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "// run the following cell if you only want to delete or mark for delection a Kafka topic"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to Delete Topic `spatial1` \n",
    "```\n",
    "%%bash\n",
    "/home/kafka/kafka/bin/kafka-topics.sh --zookeeper localhost:2181 --delete --topic spatial1\n",
    "#sudo systemctl restart kafka\n",
    "#sudo systemctl status kafka\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark",
   "language": "scala",
   "name": "sparkkernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".sc",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
