{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'jars': ['/home/isam/ApproxIoT/jars/magellan-1.0.5-s_2.11.jar'], 'conf': {'spark.jars.packages': 'org.apache.spark:spark-sql-kafka-0-10_2.11:2.2.0,com.esri.geometry:esri-geometry-api:1.2.1,commons-io:commons-io:2.6', 'spark.jars.excludes': 'org.scala-lang:scala-reflect,org.apache.spark:spark-tags_2.11', 'spark.dynamicAllocation.enabled': False}, 'kind': 'spark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>18</td><td>None</td><td>spark</td><td>idle</td><td></td><td></td><td></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%configure -f\n",
    "{\n",
    "    \"jars\":[\"/home/isam/ApproxIoT/jars/magellan-1.0.5-s_2.11.jar\"],\n",
    "    \"conf\": {\n",
    "        \"spark.jars.packages\": \"org.apache.spark:spark-sql-kafka-0-10_2.11:2.2.0,com.esri.geometry:esri-geometry-api:1.2.1,commons-io:commons-io:2.6\",\n",
    "        \"spark.jars.excludes\": \"org.scala-lang:scala-reflect,org.apache.spark:spark-tags_2.11\",\n",
    "        \"spark.dynamicAllocation.enabled\": false\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>19</td><td>None</td><td>spark</td><td>idle</td><td></td><td></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "/*\n",
    " * Author: Isam Al Jawarneh\n",
    " * Updated: October 25, 2022\n",
    " * Purpose: PREPROCESSING: the following notebook generates geohashes for millions of georeferenced data points \n",
    " *         , in addition to corresponding neighborhoods and stores the tuples in a CSV file for further processing \n",
    " */"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "//importing required libraries --- Magellan is a local jar file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import util.control.Breaks._\n",
      "import org.apache.spark.sql.streaming.StreamingQueryListener\n",
      "import org.apache.spark.sql.functions.col\n",
      "import org.apache.spark.sql.types._\n",
      "import org.apache.spark.rdd.RDD\n",
      "import org.apache.spark.SparkContext\n",
      "import org.apache.spark.SparkConf\n",
      "import org.apache.spark.sql.SparkSession\n",
      "import org.apache.spark.sql.types._\n",
      "import org.apache.spark.sql.SQLImplicits\n",
      "import org.apache.spark.sql.functions.from_json\n",
      "import org.apache.spark.sql.functions._\n",
      "import org.apache.spark.sql.DataFrame\n",
      "import org.apache.spark.sql.Dataset\n",
      "import org.apache.spark.sql.ForeachWriter\n",
      "import magellan._\n",
      "import magellan.index.ZOrderCurve\n",
      "import magellan.{Point, Polygon}\n",
      "import org.apache.spark.sql.magellan.dsl.expressions._\n",
      "import org.apache.spark.sql.Row\n",
      "import org.apache.spark.sql.types._\n",
      "import org.apache.spark.sql.SparkSession\n",
      "import org.apache.spark.sql.streaming.OutputMode\n",
      "import org.apache.spark.sql.types.{DoubleType, StringType, StructField, StructType}\n",
      "import org.apache.spark.sql.streaming._\n",
      "import org.apache.spark.sql.streaming.Trigger\n",
      "import org.apache.spark.sql.execution.streaming.MemoryStream\n",
      "import org.apache.spark.sql.functions.{collect_list, collect_set}\n",
      "import org.apache.spark.sql.SQLContext\n",
      "import org.apache.log4j.{Level, Logger}\n",
      "import scala.collection.mutable\n",
      "import scala.concurrent.duration.Duration\n",
      "import java.io.{BufferedWriter, FileWriter}\n",
      "import org.apache.commons.io.FileUtils\n",
      "import java.io.File\n",
      "import scala.collection.mutable.ListBuffer\n",
      "import java.time.Instant\n",
      "import org.apache.spark.sql.DataFrame\n",
      "import org.apache.kafka.clients.consumer.ConsumerRecord\n",
      "import org.apache.kafka.common.serialization.StringDeserializer\n"
     ]
    }
   ],
   "source": [
    "import util.control.Breaks._\n",
    "import org.apache.spark.sql.streaming.StreamingQueryListener\n",
    "//import org.apache.spark.util.random.XORShiftRandom\n",
    "import org.apache.spark.sql.functions.col\n",
    "import org.apache.spark.sql.types._\n",
    "import org.apache.spark.rdd.RDD\n",
    "import org.apache.spark.SparkContext\n",
    "import org.apache.spark.SparkConf\n",
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.sql.types._\n",
    "import org.apache.spark.sql.SQLImplicits\n",
    "import org.apache.spark.sql.functions.from_json\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.DataFrame\n",
    "import org.apache.spark.sql.Dataset\n",
    "import org.apache.spark.sql.ForeachWriter\n",
    "import magellan._\n",
    "import magellan.index.ZOrderCurve\n",
    "import magellan.{Point, Polygon}\n",
    "\n",
    "import org.apache.spark.sql.magellan.dsl.expressions._\n",
    "import org.apache.spark.sql.Row\n",
    "import org.apache.spark.sql.types._\n",
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.sql.streaming.OutputMode\n",
    "import org.apache.spark.sql.types.{\n",
    "  DoubleType,\n",
    "  StringType,\n",
    "  StructField,\n",
    "  StructType\n",
    "}\n",
    "import org.apache.spark.sql.streaming._\n",
    "import org.apache.spark.sql.streaming.Trigger\n",
    "import org.apache.spark.sql.execution.streaming.MemoryStream\n",
    "import org.apache.spark.sql.functions.{collect_list, collect_set}\n",
    "import org.apache.spark.sql.SQLContext\n",
    "import org.apache.log4j.{Level, Logger}\n",
    "import scala.collection.mutable\n",
    "import scala.concurrent.duration.Duration\n",
    "import java.io.{BufferedWriter, FileWriter}\n",
    "import org.apache.commons.io.FileUtils\n",
    "import java.io.File\n",
    "import scala.collection.mutable.ListBuffer\n",
    "import java.time.Instant\n",
    "//import org.apache.spark.util.CollectionAccumulator\n",
    "import org.apache.spark.sql.DataFrame\n",
    "import org.apache.kafka.clients.consumer.ConsumerRecord\n",
    "import org.apache.kafka.common.serialization.StringDeserializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "schemaShenzhenShort: org.apache.spark.sql.types.StructType = StructType(StructField(id,StringType,false), StructField(lat,DoubleType,false), StructField(lon,DoubleType,false), StructField(time,StringType,false), StructField(speed,DoubleType,false))\n"
     ]
    }
   ],
   "source": [
    "//defining the schema of the data that we are reading from a CSV file\n",
    "//field names and arrangement are equivalent to those in the CSV file heading\n",
    "val schemaShenzhenShort = StructType(Array(\n",
    "    StructField(\"id\", StringType, false),\n",
    "    StructField(\"lat\", DoubleType, false),\n",
    "    StructField(\"lon\", DoubleType, false),\n",
    "    StructField(\"time\", StringType, false),\n",
    "    StructField(\"speed\", DoubleType, false)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: Int = 30\n"
     ]
    }
   ],
   "source": [
    "//parameters: geohash precision. Other common values: 25, 35\n",
    "\n",
    "val precision = 30\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "geohashUDF: org.apache.spark.sql.expressions.UserDefinedFunction = UserDefinedFunction(<function1>,ArrayType(StringType,true),Some(List(ArrayType(org.apache.spark.sql.types.ZOrderCurveUDT@139992bd,true))))\n"
     ]
    }
   ],
   "source": [
    "// user defined function for converting the ZOrderCurve into the corresponding geohash\n",
    "val geohashUDF = udf{(curve: Seq[ZOrderCurve]) => curve.map(_.toBase32())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformationStream1: org.apache.spark.sql.DataFrame = [id: int, lat: double ... 3 more fields]\n",
      "ridesGeohashed: org.apache.spark.sql.DataFrame = [id: int, lat: double ... 5 more fields]\n",
      "warning: there was one deprecation warning; re-run with -deprecation for details\n",
      "dataStream1: org.apache.spark.sql.DataFrame = [id: int, lat: double ... 6 more fields]\n",
      "transformationStream: org.apache.spark.sql.DataFrame = [id: int, lat: double ... 5 more fields]\n"
     ]
    }
   ],
   "source": [
    "//reading the CSV tuples and adding a geohash column containing geohash value for each tuple\n",
    "val transformationStream1 = spark.read.option(\"inferSchema\", \"true\").option(\"header\", \"true\").csv(\"/home/isam/Desktop/spatial/data/china/points/guang.csv\")\n",
    "\n",
    "val ridesGeohashed = transformationStream1.withColumn(\"point\", point($\"lat\",$\"lon\")).withColumn(\"index\", $\"point\" index  precision).withColumn(\"geohashArray\", geohashUDF($\"index.curve\")).select($\"id\",$\"lat\",$\"lon\",$\"time\", $\"point\",$\"geohashArray\",$\"speed\")\n",
    "val dataStream1 = ridesGeohashed.explode(\"geohashArray\", \"geohash\") { a: mutable.WrappedArray[String] => a }\n",
    "val transformationStream = dataStream1.select(\"id\",\"lat\",\"lon\",\"time\",\"point\",\"speed\", \"geohash\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- lat: double (nullable = true)\n",
      " |-- lon: double (nullable = true)\n",
      " |-- time: timestamp (nullable = true)\n",
      " |-- point: point (nullable = false)\n",
      " |-- speed: integer (nullable = true)\n",
      " |-- geohash: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transformationStream.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "transformationStream.createOrReplaceTempView(\"queryTable_static\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------+------------------+-------------------+--------------------+-----+-------+\n",
      "| id|               lat|               lon|               time|               point|speed|geohash|\n",
      "+---+------------------+------------------+-------------------+--------------------+-----+-------+\n",
      "|  0|114.03179899999999|22.524798999999998|2014-10-22 04:54:30|Point(114.0317989...|   42| ws104u|\n",
      "|  0|        114.038696|           22.5315|2014-10-22 04:54:37|Point(114.038696,...|   52| ws105j|\n",
      "+---+------------------+------------------+-------------------+--------------------+-----+-------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transformationStream.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count: org.apache.spark.sql.DataFrame = [count(speed): bigint]\n",
      "+------------+\n",
      "|count(speed)|\n",
      "+------------+\n",
      "|     1155653|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "//static: counting the number of tuples\n",
    "val count = spark.sql(\"SELECT count(speed) from queryTable_static\")\n",
    "count.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df2: org.apache.spark.sql.DataFrame = [count(DISTINCT geohash): bigint]\n"
     ]
    }
   ],
   "source": [
    "//counting distinct geohashes in the CSV file (after adding the geohash column)\n",
    "val df2 = transformationStream.select(countDistinct(\"geohash\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+\n",
      "|count(DISTINCT geohash)|\n",
      "+-----------------------+\n",
      "|1561                   |\n",
      "+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "//counting distinct geohashes in the CSV file (after adding the geohash column)\n",
    "\n",
    "df2.show(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "warning: there was one deprecation warning; re-run with -deprecation for details\n",
      "geohashedNeighborhoods: (geohashPrecision: Int, filePath: String)org.apache.spark.sql.DataFrame\n"
     ]
    }
   ],
   "source": [
    "//  a function that takes the geohash precision and the polygons GeoJSON file\n",
    "//it generates the geohash covering for all the polygons in the GeoJSON file and explodes them\n",
    "// so that for each polygon, there are many records with each record containing one geohash value of the cover\n",
    "\n",
    "def geohashedNeighborhoods(geohashPrecision: Int, filePath: String): DataFrame = \n",
    "\n",
    "{\n",
    "\n",
    "import spark.implicits._\n",
    "/*preparing the neighborhoods table (static table) .... getting geohashes covering for every neighborhood and \n",
    "exploding it, so that each neighborhood has many geohashes*/\n",
    "\n",
    "// this will be executed only one time - batch mode \n",
    "val rawNeighborhoods = spark.sqlContext.read.format(\"magellan\").option(\"type\", \"geojson\").load(filePath).select($\"polygon\", $\"metadata\"(\"NAME\").as(\"neighborhood\")).cache()\n",
    "\n",
    "val neighborhoods = rawNeighborhoods.withColumn(\"index\", $\"polygon\" index geohashPrecision).select($\"polygon\", $\"index\", \n",
    "      $\"neighborhood\").cache()\n",
    "    print(neighborhoods.count())\n",
    "\n",
    "val zorderIndexedNeighborhoods = neighborhoods.withColumn(\"index\", explode($\"index\")).select(\"polygon\", \"index.curve\", \"index.relation\",\"neighborhood\")\n",
    "val geohashedNeighborhoods= neighborhoods.withColumn(\"geohashArray\", geohashUDF($\"index.curve\"))\n",
    "\n",
    "val explodedgeohashedNeighborhoods = geohashedNeighborhoods.explode(\"geohashArray\", \"geohash\") { a: mutable.WrappedArray[String] => a }\n",
    "\n",
    "//unit testing: explodedgeohashedNeighborhoods.show(10)\n",
    "\n",
    "\n",
    "explodedgeohashedNeighborhoods\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7geohashedNeigboors: org.apache.spark.sql.DataFrame = [polygon: polygon, index: array<struct<curve:zordercurve,relation:string>> ... 3 more fields]\n"
     ]
    }
   ],
   "source": [
    "//reading the neighborhoods file on the form of GeoJSON\n",
    "val geohashedNeigboors = geohashedNeighborhoods(precision,\"/home/isam/Desktop/spatial/data/china/neigh_Douglas_shapefiles/converted2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+------------+--------------------+-------+\n",
      "|             polygon|               index|neighborhood|        geohashArray|geohash|\n",
      "+--------------------+--------------------+------------+--------------------+-------+\n",
      "|magellan.Polygon@...|[[ZOrderCurve(113...|         宝安区|[ws0chk, ws0chm, ...| ws0buz|\n",
      "|magellan.Polygon@...|[[ZOrderCurve(113...|         宝安区|[ws0chk, ws0chm, ...| ws0bzp|\n",
      "+--------------------+--------------------+------------+--------------------+-------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "//dropping duplicate values\n",
    "geohashedNeigboors.dropDuplicates(\"geohash\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "newDF: org.apache.spark.sql.DataFrame = [polygon: polygon, index: array<struct<curve:zordercurve,relation:string>> ... 3 more fields]\n"
     ]
    }
   ],
   "source": [
    "//change the name to remove the ambiguity that may appear as both points and polygons have geohash field name\n",
    "//so, we name the geohash field in the neighborhoods file as geohashn (for geohash neighborhood)\n",
    "val newDF = geohashedNeigboors.withColumnRenamed(\"geohash\",\"geohashn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samplepointDF_SSS_join: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [id: int, lat: double ... 10 more fields]\n"
     ]
    }
   ],
   "source": [
    "//applying the filter-and-refinement approach from Spark's Magellan to join points and polygons\n",
    "// returning to which polygon (neighborhood, district, etc.,) each point belongs\n",
    "// MBR-join then point-in-polygon test for candidates\n",
    "val samplepointDF_SSS_join = transformationStream.join(newDF,newDF(\"geohashn\")===transformationStream(\"geohash\")).where($\"point\" within $\"polygon\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df3: org.apache.spark.sql.DataFrame = [count(DISTINCT geohashn): bigint]\n"
     ]
    }
   ],
   "source": [
    "//counting distinct geohash values in the neighborhoods file\n",
    "//in other terms, how much are the geohashes covering the study area (all polygons)\n",
    "val df3 = samplepointDF_SSS_join.select(countDistinct(newDF(\"geohashn\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+\n",
      "|count(DISTINCT geohashn)|\n",
      "+------------------------+\n",
      "|1420                    |\n",
      "+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3.show(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res53: Long = 1153389\n"
     ]
    }
   ],
   "source": [
    "//how many points do we have after the join in the new dataframe, consisting of the geohash value and the \n",
    "//neighborhood of each tuple from the CSV file\n",
    "samplepointDF_SSS_join.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- lat: double (nullable = true)\n",
      " |-- lon: double (nullable = true)\n",
      " |-- time: timestamp (nullable = true)\n",
      " |-- point: point (nullable = false)\n",
      " |-- speed: integer (nullable = true)\n",
      " |-- geohash: string (nullable = true)\n",
      " |-- polygon: polygon (nullable = true)\n",
      " |-- index: array (nullable = false)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- curve: zordercurve (nullable = false)\n",
      " |    |    |-- relation: string (nullable = false)\n",
      " |-- neighborhood: string (nullable = true)\n",
      " |-- geohashArray: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- geohashn: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// the new schema shows that the new dataframe contains the geohash value and the neighborhood for each tuple \n",
    "\n",
    "samplepointDF_SSS_join.printSchema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "// selecting the same fields (corresponding to the original file CSV schema ) and adding the geohash field\n",
    "val printDF = samplepointDF_SSS_join.select($\"id\",$\"lat\",$\"lon\",$\"time\",$\"speed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// selecting part of the dataframe (containing the new geohash and neighborhood) to store in a CSV file\n",
    "val printDF = samplepointDF_SSS_join.select($\"id\",$\"lat\",$\"lon\",$\"time\",$\"speed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "printDF.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "printDF.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "//samplepointDF_SSS_join.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "//storing the resulting dataframe in a new CSV file\n",
    "// to serve it to another stage in the analytics pipeline\n",
    "printDF.coalesce(1).write.option(\"header\",true).csv(\"/home/isam/Desktop/spatial/data/china/datacsv/data2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark",
   "language": "",
   "name": "sparkkernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
